---
title: "百模大战正酣，看看国内大模型谁更了解 K8S？（一）"
summary: "MiniMax、ChatGLM、Azure OpenAI(GPT3.5-16k)篇"
authors: ["guoxudong"]
tags: ["AIGC", "LLM", "AI"]
categories: ["AI"]
date: 2023-08-09T09:23:29+08:00
lastmod: 2023-08-09T09:23:29+08:00
draft: false
type: blog
image: "https://cdn.suuny0826.com/image/2023-08-09-llm-k8s.png"
---
## 前言

自 2022 年 11 月 ChatGPT 强势问世，仅两个月内便取得月均1亿以上的活跃用户，科技界瞬间掀起滔天巨浪。国内各大互联网企业迅速行动，纷纷宣告自家大语言模型即将问世。其中，不乏像[智谱 AI](https://zhipuai.cn) 这样由国内顶尖大学技术成果转化而来的公司，以及 [MiniMax](https://api.minimax.chat) 这样由人工智能领域大佬二次创业成立的创业公司。一时间，“大模型”这个词成为了媒体以及各路科技创业者口中最常被提及的词语。2023 年初，各大公司纷纷宣布将会推出自己的大模型，投入了大量的人力物力，以期在这场”百模大战“中脱颖而出。

距离各大公司宣布推出自己的大模型已经过去了半年，各大公司的大模型也陆续上线。在这半年中，由于工作需求，我调研了国内各家厂商推出的大模型。虽然效果不如初见 GPT-4 那样惊艳，但大多数都不错，效果与 GPT-3.5 相当，有些对于中文的支持甚至要好于 GPT-3.5。

作为一名每天都要与 K8S 打交道的 “YAML 工程师”，我非常希望能够找到一个更懂 K8S 的大模型，这样在遇到问题时，我就可以通过 AI 的方式快速解决问题了。于是我就开始了我的调研之旅，通过一系列 K8S 相关的问题，来测试各个大模型对 K8S 的理解程度。

## 大模型介绍

由于大模型数量较多，一次本篇文章只测评三个大模型，后续篇章会陆续更新其他大模型的测评结果。本次介绍的三个大模型分别为：**MiniMax**、**ChatGLM**、**Azure OpenAI**(GPT3.5-16k)。

首篇选择这三个大模型的原因是笔者调研的所有大模型中，**MiniMax** 和 **ChatGLM** 是其中比较特别的，其产品及商业运营模式是与 ChatGPT 最接近的，他们都是率先开放了 API 模式并提供了相应的基于 token 数的收费模式，而非像其他大模型那样首先开放 web 聊天页面的模式且不提供清晰的收费模式。Azure OpenAI 则是作为一个对照组，为其他大模型回答的结果提供一个标准。

### ChatGLM

ChatGLM 是一个人工智能助手，基于清华大学 KEG 实验室与智谱 AI 于 2023 年联合训练的语言模型 GLM-130B 开发而成。ChatGLM 的任务是针对用户的问题和要求提供适当的答复和支持。

### MiniMax

MiniMax 提供的 API 以极简的形式供企业用户或企业开发者调用，同时针对不同的行业及场景进行不同的能力抽象和封装，最大限度的降低使用者的开发复杂性，快速在目标场景中验证价值并进行生产部署。

## 限制

本次测试使用的 prompt 为中文，问题数量为 21 个，均为一些基础问题，也是面试中常被问到的问题。回答结果将以 `.csv` 格式上传至 [GitHub](https://github.com/sunny0826/llms-k8s-pk),以方便大家查看。如果有其他问题需要添加，也可以直接[提交 Issue](https://github.com/sunny0826/llms-k8s-pk/issues/new),回答内容会及时更新在 GitHub 上。需要注意的是，由于每次回答的结果都可能有所不同，因此测试结果仅供参考。此外，由于各大模型目前仍处于测试阶段，因此测试结果可能与正式上线的结果存在差异。

## 问题

本次测评的问题包括一些基础问题，也是一些经常被问到的问题，这些问题涉及的范围广泛，旨在测试大模型对 K8S 的理解程度。以下是准备好的 prompt：

1. 请简要解释一下什么是云原生?
2. Kubernetes 和 Docker 的关系是什么?
3. 请描述 Kubernetes 的主要组件及其作用?
4. 解释一下 Kubernetes 的部署策略有哪些?
5. 如何在 Kubernetes 中实现服务发现和负载均衡?
6. 请描述 Kubernetes 的 Master 节点组件。
7. 请描述 Kubernetes 的 Node 节点组件。
8. 请描述 Pod 的生命周期。
9. K8S 中什么是 Deployment?有什么特点?
10. K8S 中什么是 Service?常见的Service类型有哪些?
11. 怎么访问 Kubernetes 集群中的应用?
12. 怎么实现 Kubernetes 的负载均衡?
13. 怎么实现 Kubernetes 的滚动更新?
14. 怎么实现 Kubernetes 的回滚操作?
15. 怎么配置 Kubernetes 的资源请求和限制?
16. 怎么配置 Kubernetes 的健康检查?
17. 怎么配置 Kubernetes 的就绪检查?
18. 怎么配置 Kubernetes 的主机亲和性?
19. 怎么配置 Kubernetes 的污点和容忍?
20. 怎么给 Kubernetes 集群添加存储?
21. 怎么监控 Kubernetes 集群的状态?

## 结果

详细测评内容请查看 [GitHub repo](https://github.com/sunny0826/llms-k8s-pk)。从结果来看，三个大模型的表现都非常好，除了一些小问题外，基本上都能够回答出正确的答案。其中，MiniMax 的回答结果最为准确，几乎没有错误的回答；ChatGLM 的回答结果也非常不错，只有一个问题回答的不太准确。测评的结果让笔记非常震惊，不仅是 OpenAI 其他的大模型对于 K8S 的理解情况要远远高于逾期，回答的内容不仅中英文结合得当，而且对于很多专有名词的翻译和理解都非常准确，这让笔者对于大模型的理解能力有了更深的认识。

详细测评内容也可见腾讯文档，链接：<https://docs.qq.com/sheet/DY1VUTmJjaG1veFNW>

## 结语

本篇文章主要介绍了三个大模型：MiniMax、ChatGLM、Azure OpenAI(GPT3.5-16k)，并通过一系列 K8S 相关的问题，来测试各个大模型对 K8S 的理解程度。从结果来看，三个大模型的表现都非常好，除了一些小问题外，基本上都能够回答出正确的答案。但这些内容都是一些基本的问题，对于一些更加复杂的问题，需要对每个 prompt 进行更精细的调整，也能得到准确的结果，这也对使用大模型的人提出了更高的要求。
